---
title: "supervised.learning.revenue.predictor.Rmd"
output: html_document
---

The coefficients from the winning model (defined by error matrix results below) are used in the professional services predictor algorithm (not shown in this file), where the resultant log odds ratio is converted into a win probability and then multiplied by each opportunity's professional services value to arrive at an expected PS value for each opportunity. This exact same algorithm can be used to predict bookings as well.

```{r}
library(readr)

# load Appended_Opps.csv
forecast_input <- read.csv(file = 'Appended_Opps.csv')
```

```{r}
# Looking for null values to clean for regression 
sum(is.na(forecast_input$Forecast_Outcome))
sum(is.na(forecast_input$Forecast_Type))
sum(is.na(forecast_input$Team))
sum(is.na(forecast_input$Opp_Type))
sum(is.na(forecast_input$Forecast_Week))

```
```{r}

# This code block filters the dataset

# Library needed to use "filter" function
library(dplyr)

# Getting rid of Other because irrelevant. Removing Transactions because they represent a disproportionate number of opportunities which skews the data in a very different way than non-transactional teams do.
forecast_filtered1 <- filter(forecast_input,
  !(Team %in% c("Transactions", "Other")))
  
# Only keeping pipeline stages
forecast_filtered2 <- filter(forecast_filtered1,
  !(Stage %in% c("Migrate", "Decom", "Won")))

# Removing "Omitted" forecast category
forecast_filtered3 <- filter(forecast_filtered2,
  !(Forecast_Type %in% c("Omitted")))

  
# Count rows in each dataframe to ensure the filtering is correct
nrow(forecast_filtered1)
nrow(forecast_filtered2)
nrow(forecast_filtered3)

# Remove all prior dataframes from the global environment (for cleanliness)
rm(forecast_filtered1, forecast_filtered2)

```

```{r}
# Turning target variable (Forecast_Outcome) into 1 or 0
reg_input <- within(forecast_filtered3,   {
  Target_Forecast_Outcome <- NA
    Target_Forecast_Outcome[Forecast_Outcome==
    "Win"] <- 1
    Target_Forecast_Outcome[Forecast_Outcome==
    "Loss/Push"] <- 0
})

# Clean workspace
rm(list=setdiff(ls(), "reg_input"))

```

```{r}
# Create interaction terms
interaction.terms <- as.data.frame(model.matrix(~
      (Enterprise + SMB + Mid-Market + Outbound + Networks + Government + Mexico - 1)*
      (Cross.Sell + New + Upgrade + Upsell + Migration + Decommission + Recontract - 1) * (Lifespan.30 + Lifespan.60 - 1), reg.input))


# Remove dummy variables (since they already exist in the reg.input dataset)
# Remove three-way interaction (only 2 way interactions with Segment)
library(dplyr)
only.interaction.terms <- subset(select(interaction.terms, 16:69))

# Append interaction term dataset to reg.input
appended.reg <- cbind(reg.input, only.interaction.terms)

# Clean workspace
rm(list=setdiff(ls(), c('reg.input','appended.reg')))

# Getting rid of the nested "X" column
appended.reg = subset(appended.reg, select = -c(x) )
```

```{r}
# The following oversampling is required to decrease the volume of non-events so that proportion of events and non-events gets less skewed, since target events spend less time being "visible" in the source dataset due to their much shorter lifespans.


#oversample for minority class to be 40% all observations (57,453 observations)
forty.upsample.minority <- appended.reg[sample( which( appended.reg$Target_Forecast_Outcome==1), 48821, replace = TRUE), ]

#oversample for minority class to be 30% of all observations (36,934 observations)
thirty.upsample.minority <- appended.reg[sample( which( appended.reg$Target_Forecast_Outcome==1), 28302, replace = TRUE), ]

#oversample for minority class to be 20% of all observations (21,545 observations)
twenty.upsample.minority <- appended.reg[sample( which( appended.reg$Target_Forecast_Outcome==1), 12913, replace = TRUE), ]

#oversample for minority class to be 15% of all observations (15,208 observations)
fifteen.upsample.minority <- appended.reg[sample( which( appended.reg$Target_Forecast_Outcome==1), 6576, replace = TRUE), ]


#combine original dataset with the new oversampled minority datasets
forty.balanced.reg <- rbind(appended.reg, forty.upsample.minority)

thirty.balanced.reg <- rbind(appended.reg, thirty.upsample.minority)

twenty.balanced.reg <- rbind(appended.reg, twenty.upsample.minority)

fifteen.balanced.reg <- rbind(appended.reg, fifteen.upsample.minority)

#count opp.won.target columns to see if sampling worked correctly
table(forty.balanced.reg$Target_Forecast_Outcome)
table(thirty.balanced.reg$Target_Forecast_Outcome)
table(twenty.balanced.reg$Target_Forecast_Outcome)
table(fifteen.balanced.reg$Target_Forecast_Outcome)

# Clean workspace
rm(list=setdiff(ls(), c('reg.input','appended.reg', 'forty.balanced.reg', 'thirty.balanced.reg', 'twenty.balanced.reg', 'fifteen.balanced.reg')))
```

```{r}
# Load required library
library(caret)

# Set seed for reproducibility
set.seed(44)

# Step 1: Partition the data
trainIndex <- createDataPartition(twenty.balanced.reg$Target_Forecast_Outcome, p = 0.75, list = FALSE)
train <- twenty.balanced.reg[trainIndex, ]
temp <- twenty.balanced.reg[-trainIndex, ]
validIndex <- createDataPartition(temp$Target_Forecast_Outcome, p = 0.5, list = FALSE)
valid <- temp[validIndex, ]
test <- temp[-validIndex, ]

# Step 2: Set variables as categorical
twenty.balanced.reg <- as.data.frame(lapply(twenty.balanced.reg, as.factor))

# Step 3: Create logistic regression 

# IMPORTANT - make sure to drop unnecessary columns that won't be used in the regression before running this part, as you'll receive a "cannot allocate vector of size x" error
logistic_model <- train(
  Target_Forecast_Outcome ~ ., 
  data = twenty.balanced.reg[trainIndex, ], 
  method = "glm",
  trControl = trainControl(method = "none"),
  family = "binomial"
)

# Step 4: Create error matrix
predictions <- predict(logistic_model, newdata = twenty.balanced.reg[-trainIndex, ], type = "raw")
confusionMatrix(predictions, twenty.balanced.reg[-trainIndex, ]$Target_Forecast_Outcome)

```

```{r}
# Same as above, but for the forty.balanced.reg dataframe. Whichever 

# Load required library
library(caret)

# Set seed for reproducibility
set.seed(44)

# Step 1: Partition the data
trainIndex <- createDataPartition(forty.balanced.reg$Target_Forecast_Outcome, p = 0.75, list = FALSE)
train <- forty.balanced.reg[trainIndex, ]
temp <- forty.balanced.reg[-trainIndex, ]
validIndex <- createDataPartition(temp$Target_Forecast_Outcome, p = 0.5, list = FALSE)
valid <- temp[validIndex, ]
test <- temp[-validIndex, ]

# Step 2: Set variables as categorical
forty.balanced.reg <- as.data.frame(lapply(forty.balanced.reg, as.factor))

# Step 3: Create logistic regression
logistic_model <- train(
  Target_Forecast_Outcome ~ ., 
  data = forty.balanced.reg[trainIndex, ], 
  method = "glm",
  trControl = trainControl(method = "none"),
  family = "binomial"
)

# Step 4: Create error matrix
predictions <- predict(logistic_model, newdata = forty.balanced.reg[-trainIndex, ], type = "raw")
confusionMatrix(predictions, forty.balanced.reg[-trainIndex, ]$Target_Forecast_Outcome)

```

I didn't include all of the different model iterations for proprietary reasons, but here's a snippet of the comparative confusion matrix results.

	- Forty.balanced.reg
		○ Version 1b (the winning model)
			§ Pos Error: 46.2% 
			§ Predicted Win: 28.1%

	- Thirty.balanced.reg
		○ Version 1
			§ Pos Error: 54.6%
			§ Predicted Win: 19.8%

		○ Version 2b
			§ Pos Error: 53%
			§ Predicted Win: 20.3%

	- Twent.balanced.reg
		○ Version 1
			§ Pos Error: 66.6%
			§ Predicted Win: 10.8%

		○ Version 2
			§ Pos Error: 66%
			§ Predicted Win: 10.8%


The final model I stuck with (forty.balanced.reg version 1b) was a variant of the "forty.balanced.reg" model that dropped many of the interaction terms (which turned out to not be all that differentiated from the base inputs), along with other dummy variables that lacked predictive power. The coefficients from this final model were then plugged into the forecast algorithim as I discussed in the introduction.
